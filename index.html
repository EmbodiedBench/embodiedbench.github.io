<!DOCTYPE html>
<html>
<head>
    <title>EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</title>
    
    <!-- Favicon -->
    <link rel="icon" href="website/img/embodied-logo.png" type="image/png">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Chart.js and plugins -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <!-- Fonts -->
    <!-- Montserrat for titles, Open Sans for body -->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans&display=swap" rel="stylesheet">

    <!-- Bulma & Additional CSS -->
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="website/css/fontawesome.all.min.css"> 

    <!-- jQuery & Bulma scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="website/javascript/bulma-carousel.min.js"></script> 
    <script src="website/javascript/bulma-slider.min.js"></script>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
            crossorigin="anonymous"></script>

    <!-- Tabulator -->
    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>

    <!-- FontAwesome -->
    <script defer src="website/javascript/fontawesome.all.min.js"></script>

    <!-- Custom CSS & JS -->
    <link rel="stylesheet" href="website/css/index.css">
    <script src="website/javascript/benchmark_table.js" type="module"></script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C7GJ4FYMY9');
    </script>

    <!-- MathJax -->
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript">
      MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] }
      });
    </script>

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>

    <script>
      // Toggle sections
      document.addEventListener('DOMContentLoaded', function () {
          var toggles = document.querySelectorAll('.toggle-section');
          toggles.forEach(function (toggle) {
              toggle.addEventListener('click', function () {
                  var content = document.getElementById(toggle.getAttribute('aria-controls'));
                  var toggleIcon = toggle.children[1].children[0];
                  content.classList.toggle('is-active');
                  if (content.classList.contains('is-active')) {
                      toggleIcon.style.transition = 'transform 0.3s ease';
                      toggleIcon.style.transform = 'rotate(180deg)';
                  } else {
                      toggleIcon.style.transition = 'transform 0.3s ease';
                      toggleIcon.style.transform = 'rotate(0deg)';
                  }
              });
          });
      });
    </script>

    <style>
      /* Global fonts */
      html, body {
        font-family: 'Open Sans', sans-serif;
      }

      /* Titles use Montserrat */
      h1, h2, h3, .title, .subtitle {
        font-family: 'Montserrat', sans-serif;
      }

      /* Reduce extra spacing between sections */
      .hero.teaser {
        margin-bottom: 0 !important;
        padding-bottom: 1.5rem !important;
      }
      .section {
        padding-top: 2rem !important;
        padding-bottom: 2rem !important;
      }

      /* MAIN TITLE */
      .title-primary {
        font-size: 2.2rem;  /* Larger title */
        text-align: center;
        color: black;
      }
      .title-secondary {
        font-size: 1.5rem;
        text-align: center;
        margin-top: 0.5rem;
        margin-bottom: 1rem;
        color: black;
      }

      /* Abstract styling */
      #abstract-title {
        text-align: center;
        color: black;
        margin-bottom: 1rem;
      }
      .abstract-content {
        color: black;
        text-align: justify;
      }

      /* Table styling */
      .table-figure {
        margin-bottom: 2rem;
      }
      table.is-bordered {
        border: 1px solid #ccc;
      }
      table.is-bordered th,
      table.is-bordered td {
        border: 1px solid #ccc;
        font-size: 0.85rem; /* Smaller table text */
      }
      table thead tr {
        background: #f0f0f0;
      }

      /* Center "Proprietary MLLMs" / "Open-Source MLLMs" */
      .model-category {
        text-align: center;
        background: #e8e8e8;
        font-weight: 700;
      }

      /* Figure & Table caption smaller font */
      .figure-caption {
        font-size: 0.8rem; /* smaller than before */
        font-style: italic;
        text-align: center;
        margin-top: 0.3rem;
        color: #333;
      }

      .figure-image {
        max-width: 100%;
        height: auto;
      }

      /* Larger image for error analysis */
      .error-analysis-img {
        width: 90%;
        height: auto;
        margin: 0 auto;
        display: block;
      }
    </style>
</head>

<body>
  <!-- Title Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- MAIN TITLE (logo + "EmbodiedBench:") -->
            <h1 class="title-primary">
              <img src="website/img/embodied-logo.png"
                   alt="logo"
                   style="vertical-align: middle; margin-right: 10px;"
                   width="90"
                   height="90" />
              EmbodiedBench:
            </h1>

            <!-- SUBTITLE -->
            <h2 class="title-secondary">
              Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents
            </h2>

            <!-- Author list -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yangrui2015.github.io/" target="_blank">Rui Yang</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Hanyang_Chen3" target="_blank">Hanyang Chen</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jyzhang1208.github.io/" target="_blank">Junyu Zhang</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/markzihaozhao/" target="_blank">Mark Zhao</a><sup>*3</sup>,
              </span>
              <span class="author-block">
                <a href="https://qiancheng0.github.io/" target="_blank">Cheng Qian</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jameskrw.github.io/" target="_blank">Kangrui Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://qinengwang-aiden.github.io/" target="_blank">Qineng Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/teja-koripella/" target="_blank">Teja Venkat Koripella</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/marziyeh-movahedi-439b7284/" target="_blank">Marziyeh Movahedi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://limanling.github.io/" target="_blank">Manling Li</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://blender.cs.illinois.edu/hengji.html" target="_blank">Heng Ji</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.huan-zhang.com/" target="_blank">Huan Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://tongzhang-ml.org/" target="_blank">Tong Zhang</a><sup>1</sup>
              </span>
            </div>

            <!-- Affiliation legend -->
            <div class="is-size-5 publication-authors" style="margin-top:1em;">
              <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign (UIUC),</span>
              <span class="author-block"><sup>2</sup>Northwestern University,</span>
              <span class="author-block"><sup>3</sup>University of Toronto</span>
            </div>

            <!-- Equal contribution note -->
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution</span>
            </div>

            <!-- PDF / Arxiv / Code links -->
            <div class="column has-text-centered" style="margin-top: 1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.07166" class="btn btn-outline-dark" role="button" target="_blank">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a> &nbsp;&nbsp;
              </span>
              <span class="link-block">
                <a href="https://github.com/embodied-agent-eval/embodied-agent-eval" class="btn btn-outline-dark" role="button" target="_blank">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a> &nbsp;&nbsp;
              </span>
            </div>

          </div> <!-- column -->
        </div> <!-- columns -->
      </div> <!-- container -->
    </div> <!-- hero-body -->
  </section>

  <!-- Teaser Video Section -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body" style="padding-top:1rem;">
        <video id="teaser" autoplay muted loop playsinline controls width="100%">
          <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-0-overview.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section>

  <!-- Example sub-sections with videos -->
  <section class="section" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h3 class="title is-5">Goal Interpretation</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-1-goal.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Subgoal Decomposition</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-2-subgoal.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Action Sequencing</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-3-action.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Transition Modeling</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-4-transition.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
      <div class="columns is-centered has-text-centered" style="margin-top:1rem;">
        <div class="column">
          <button class="button is-light toggle-section" aria-controls="full_demo_video">
            <span class="icon">
              <i class="fas fa-play"></i>
            </span>
            <span>Watch Full Demo</span>
            <span class="icon">
              <i class="fas fa-angle-down" style="margin-left: 5px;"></i>
            </span>
          </button>
          <div id="full_demo_video" class="collapse-content" style="margin-top: 1rem;">
            <video id="demo" controls width="100%">
              <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/eai.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Single-Paragraph Abstract Section -->
  <section class="section" id="abstract" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 id="abstract-title" class="title is-3">Abstract</h2>
      <div class="columns is-centered">
        <div class="column is-eight-fifths">
          <div class="abstract-content">
            <p>
              Leveraging Multi-modal Large Language Models (MLLMs) to create <b>vision-driven embodied agents</b>
              offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have
              garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of
              comprehensive evaluation frameworks. To bridge this gap, we introduce <b>EmbodiedBench</b>, an
              extensive benchmark designed to evaluate <b>vision-driven embodied agents</b>. EmbodiedBench features:
              (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic
              tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation);
              and (2) six meticulously curated subsets evaluating essential agent capabilities like common sense
              reasoning, complex instruction following, spatial awareness, visual perception, and long-term planning.
              Through extensive experiments, we evaluate 13 leading proprietary and open-source MLLMs within
              EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level
              manipulation, with the best model, GPT-4o, scoring only on average. <b>EmbodiedBench provides a
              standardized, multifaceted evaluation platform</b> that not only highlights existing challenges
              but also offers valuable insights for advancing <b>MLLM-based embodied agents</b>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Overview Section -->
  <section class="section" id="overview" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align:center; margin-bottom:1rem;">Overview</h2>

      <div class="columns is-centered has-text-centered">
        <div class="column is-eight-fifths">
          <!-- Figure 1 -->
          <figure class="table-figure">
            <img class="figure-image" src="website/img/framework_overview.png" alt="Figure 1. Overview of EMBODIEDBENCH." />
            <figcaption class="figure-caption">
              <b>Figure 1.</b> Overview of EMBODIEDBENCH. Two key features of our benchmark: various action levels and capability-oriented evaluation.
            </figcaption>
          </figure>

          <p style="text-align:justify; color:black;">
            We propose <b>EmbodiedBench</b> to bridge the gap between text-centric benchmarks and
            <b>real-world</b>, <b>vision-driven</b> tasks. Our benchmark evaluates
            <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(MLLMs)</b>
            on both <b>high-level</b> semantic planning (e.g., “find a tomato and place it in the sink”)
            and <b>low-level</b> atomic actions (e.g., discretized robot arm actions or atomic movement commands).
            By unifying <b>multiple</b> <b>action</b> <b>levels</b> and <b>capability-based</b> <b>subsets</b>
            in a single platform, <b>EmbodiedBench</b> offers a <b>direct comparison</b> of how effectively MLLMs
            tackle real-world complexity, ranging from everyday semantic tasks to <b>fine-grained</b> atomic commands,
            all while leveraging <b>vision</b> as a key input. Our extensive experiments with 13 state-of-the-art models
            reveal specific strengths (e.g., high-level planning) and consistent weaknesses (especially in low-level
            manipulation and long-horizon tasks).
          </p>

          <p style="text-align:justify; margin-top:1em; color:black;">
            <b>EmbodiedBench</b> is a <b>vision-driven</b> benchmark that enforces <b>hierarchical</b>
            <b>action</b> <b>representation</b> in MLLMs, from <b>high-level</b> commands
            (e.g., <span style="color:green; font-size:0.9em; font-style: italic;">"find a HandTowel"</span>)
            to <b>low-level</b> 7D vectors
            (<span style="color:green; font-size:0.9em; font-style: italic;">[X, Y, Z, Roll, Pitch, Yaw, Gripper]</span>).
            Its four environments, <b>EB-ALFRED</b>, <b>EB-Habitat</b>, <b>EB-Navigation</b>,
            and <b>EB-Manipulation</b>, span tasks ranging from abstract household directives
            to precise robotic control. Each environment’s tasks are organized into six
            <b>capability-oriented</b> <b>subsets</b> (Base, Common Sense, Complex Instruction, Spatial Awareness,
            Visual Appearance, Long Horizon). Altogether, 13 <b>leading</b> <b>MLLMs</b>, including
            <b>GPT-4o</b>, <b>Claude-3.5-Sonnet</b>, <b>Gemini</b>, <b>Llama-3.2</b> <b>Vision</b>,
            <b>InternVL2.5</b>, and <b>Qwen2-VL</b>, are evaluated, providing a comprehensive look at how
            well these models handle <b>semantic</b> <b>planning</b> and <b>fine-grained</b> manipulation
            in real-world-inspired scenarios.
          </p>
        </div>
      </div>

      <div class="columns is-centered has-text-centered" style="margin-top:2em;">
        <div class="column is-eight-fifths">
          <!-- Figure 2 -->
          <figure class="table-figure">
            <img class="figure-image" src="website/img/planner_pipeline.png" alt="Figure 2. Vision-driven agent pipeline used in EMBODIEDBENCH." />
            <figcaption class="figure-caption">
              <b>Figure 2.</b> Vision-driven agent pipeline used in EMBODIEDBENCH.
            </figcaption>
          </figure>

          <p style="text-align:justify; margin-top:1em; color:black;">
            Our <b>vision-driven</b> pipeline fuses visual perception, language instructions, and in-context examples
            to guide <b>MLLM-based</b> agents in both <b>high-level</b> and <b>low-level</b> tasks.
            The decision module sequentially processes (1) the current egocentric image, (2) the user goal,
            (3) a valid skill or action format, and (4) in-context demonstration, yielding a
            <b>Language Plan</b> that’s converted into an <b>Executable</b> <b>Plan</b>.
            After each execution, the environment’s feedback (collisions, success signals) triggers reflection
            and potential replanning, enabling a single model to handle everything from
            <b>semantic</b> tasks to <b>fine-grained</b> robotic controls.
          </p>
        </div>
      </div>

    </div>
  </section>

  <!-- Leaderboard and Findings Section -->
  <section class="section" id="leaderboard-findings" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align:center; margin-bottom:1rem;">Leaderboard and Findings</h2>
      <!-- One blank line of space -->
      <div style="height:1rem;"></div>

      <!-- Table 2 -->
      <figure class="table-figure">
        <table class="table is-bordered is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th rowspan="2" style="vertical-align: middle; text-align:center;">Model</th>
              <th colspan="7" style="text-align:center;">EB-ALFRED (6 subsets)</th>
              <th colspan="7" style="text-align:center;">EB-HABITAT (6 subsets)</th>
            </tr>
            <tr>
              <th>Avg</th><th>Base</th><th>Common</th><th>Complex</th><th>Visual</th><th>Spatial</th><th>Long</th>
              <th>Avg</th><th>Base</th><th>Common</th><th>Complex</th><th>Visual</th><th>Spatial</th><th>Long</th>
            </tr>
          </thead>
          <tbody>
            <tr><td colspan="15" class="model-category">Proprietary MLLMs</td></tr>
            <tr>
              <td>GPT-4o</td>
              <td>56.3</td><td>64</td><td>54</td><td>68</td><td>46</td><td>52</td><td>54</td>
              <td>59.0</td><td>86</td><td>44</td><td>56</td><td>68</td><td>36</td><td><b>64</b>
            </tr>
            <tr>
              <td>GPT-4o-mini</td>
              <td>23.0</td><td>34</td><td>28</td><td>36</td><td>24</td><td>16</td><td>0</td>
              <td>32.7</td><td>74</td><td>22</td><td>32</td><td>22</td><td>32</td><td>14</td>
            </tr>
            <tr>
              <td>Claude-3.5-Sonnet</td>
              <td><b>64.0</b></td><td><b>72</b></td><td><b>66</b></td><td><b>76</b></td><td><b>60</b></td><td><b>58</b></td><td>52</td>
              <td><b>68.0</b></td><td><b>96</b></td><td><b>68</b></td><td><b>78</b></td><td>70</td><td><b>38</b></td><td>58</td>
            </tr>
            <tr>
              <td>Gemini-1.5-Pro</td>
              <td>62.3</td><td>70</td><td>64</td><td>72</td><td>58</td><td>52</td><td><b>58</b></td>
              <td>56.3</td><td>92</td><td>52</td><td>48</td><td>56</td><td><b>38</b></td><td>52</td>
            </tr>
            <tr>
              <td>Gemini-2.0-flash</td>
              <td>52.3</td><td>62</td><td>48</td><td>54</td><td>46</td><td>46</td><td><b>58</b></td>
              <td>42.3</td><td>82</td><td>38</td><td>38</td><td>36</td><td>34</td><td>26</td>
            </tr>
            <tr>
              <td>Gemini-1.5-flash</td>
              <td>39.3</td><td>44</td><td>40</td><td>56</td><td>42</td><td>26</td><td>28</td>
              <td>39.3</td><td>76</td><td>32</td><td>48</td><td>36</td><td>32</td><td>12</td>
            </tr>
            <tr>
              <td>GPT-4o (Lang)</td>
              <td>58.0</td><td>62</td><td>64</td><td>70</td><td>52</td><td>46</td><td>54</td>
              <td>56.0</td><td>82</td><td>52</td><td>58</td><td><b>74</b></td><td>34</td><td>36</td>
            </tr>
            <tr>
              <td>GPT-4o-mini (Lang)</td>
              <td>31.3</td><td>42</td><td>36</td><td>46</td><td>30</td><td>20</td><td>14</td>
              <td>36.7</td><td>82</td><td>30</td><td>34</td><td>30</td><td>30</td><td>14</td>
            </tr>

            <tr><td colspan="15" class="model-category">Open-Source MLLMs</td></tr>
            <tr>
              <td>Llama-3.2-90B-Vision-Ins</td>
              <td>32.0</td><td>38</td><td>34</td><td>44</td><td>28</td><td>32</td><td>16</td>
              <td>40.3</td><td>94</td><td>24</td><td>50</td><td>32</td><td>28</td><td>14</td>
            </tr>
            <tr>
              <td>Llama-3.2-11B-Vision-Ins</td>
              <td>13.7</td><td>24</td><td>8</td><td>16</td><td>22</td><td>6</td><td>6</td>
              <td>25.0</td><td>70</td><td>16</td><td>28</td><td>10</td><td>20</td><td>6</td>
            </tr>
            <tr>
              <td>InternVL2 5-78B</td>
              <td><b><u>37.7</u></b></td><td>38</td><td><b><u>34</u></b></td><td>42</td><td><b><u>34</u></b></td><td><b><u>36</u></b></td><td><b><u>42</u></b></td>
              <td><b><u>49.0</u></b></td><td>80</td><td><b><u>42</u></b></td><td><b><u>56</u></b></td><td><b><u>58</u></b></td><td>30</td><td>28</td>
            </tr>
            <tr>
              <td>InternVL2 5-38B</td>
              <td>23.3</td><td>36</td><td>30</td><td>36</td><td>22</td><td>14</td><td>26</td>
              <td>38.3</td><td>60</td><td>28</td><td>48</td><td>34</td><td><b><u>32</u></b></td><td><b><u>28</u></b></td>
            </tr>
            <tr>
              <td>InternVL2 5-8B</td>
              <td>2.0</td><td>4</td><td>6</td><td>2</td><td>0</td><td>0</td><td>0</td>
              <td>11.3</td><td>36</td><td>4</td><td>0</td><td>10</td><td>16</td><td>2</td>
            </tr>
            <tr>
              <td>Qwen2-VL-72B-Ins</td>
              <td>33.7</td><td><b><u>40</u></b></td><td>30</td><td>40</td><td>30</td><td>32</td><td>30</td>
              <td>35.7</td><td>70</td><td>30</td><td>36</td><td>32</td><td>28</td><td>18</td>
            </tr>
            <tr>
              <td>Qwen2-VL-7B-Ins</td>
              <td>1.7</td><td>6</td><td>0</td><td>2</td><td>0</td><td>0</td><td>2</td>
              <td>18.3</td><td>48</td><td>6</td><td>16</td><td>20</td><td>18</td><td>2</td>
            </tr>
          </tbody>
        </table>
        <figcaption class="figure-caption">
          <b>Table 2.</b> Main results of EB-ALFRED and EB-HABITAT on 6 subsets with proprietary models and open-source models.<br>
          The best proprietary model is in <b>bold</b>, and the best open-source model is in <b><u>bold underline</u></b>.
        </figcaption>
      </figure>

      <!-- Table 3 -->
      <figure class="table-figure">
        <table class="table is-bordered is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th rowspan="2" style="vertical-align: middle; text-align:center;">Model</th>
              <th colspan="5" style="text-align:center;">EB-Navigation (5 subsets)</th>
              <th colspan="6" style="text-align:center;">EB-Manipulation (5 subsets)</th>
            </tr>
            <tr>
              <th>Avg</th><th>Base</th><th>Common</th><th>Complex</th><th>Visual</th>
              <th>Avg</th><th>Base</th><th>Common</th><th>Complex</th><th>Visual</th><th>Spatial</th>
            </tr>
          </thead>
          <tbody>
            <tr><td colspan="12" class="model-category">Proprietary MLLMs</td></tr>
            <tr>
              <td>GPT-4o</td>
              <td>57.7</td><td>55.0</td><td>60.0</td><td>58.3</td><td>60.0</td>
              <td>28.9</td><td>39.6</td><td>29.2</td><td>29.2</td><td>19.4</td><td>25.0</td>
            </tr>
            <tr>
              <td>GPT-4o-mini</td>
              <td>32.8</td><td>31.7</td><td>33.3</td><td>35.0</td><td>28.3</td>
              <td>4.8</td><td>4.2</td><td>6.3</td><td>2.1</td><td>0.0</td><td>10.4</td>
            </tr>
            <tr>
              <td>Claude-3.5-Sonnet</td>
              <td>44.7</td><td>66.7</td><td>51.7</td><td>41.7</td><td>36.7</td>
              <td>26.7</td><td>37.5</td><td>16.7</td><td>29.2</td><td>19.4</td><td>22.9</td>
            </tr>
            <tr>
              <td>Gemini-1.5-Pro</td>
              <td>24.3</td><td>23.3</td><td>25.0</td><td>25.0</td><td>28.3</td>
              <td>21.1</td><td>14.6</td><td>14.6</td><td>22.9</td><td>16.7</td><td>35.4</td>
            </tr>
            <tr>
              <td>Gemini-2.0-flash</td>
              <td>48.7</td><td>63.3</td><td>65.0</td><td>50.0</td><td>51.7</td>
              <td>13.3</td><td>16.7</td><td>14.6</td><td>8.3</td><td>14.6</td><td>31.3</td>
            </tr>
            <tr>
              <td>Gemini-1.5-flash</td>
              <td>41.7</td><td>56.7</td><td>50.0</td><td>46.7</td><td>50.0</td>
              <td>5.0</td><td>9.6</td><td>14.6</td><td>4.2</td><td>8.3</td><td>10.4</td>
            </tr>
            <tr>
              <td>GPT-4o (Lang)</td>
              <td>17.4</td><td>21.7</td><td>21.7</td><td>26.7</td><td>16.7</td>
              <td>0.0</td><td>16.2</td><td>16.7</td><td>14.6</td><td>19.4</td><td>14.6</td>
            </tr>
            <tr>
              <td>GPT-4o-mini (Lang)</td>
              <td>8.3</td><td>3.3</td><td>13.3</td><td>10.0</td><td>15.0</td>
              <td>0.0</td><td>6.6</td><td>12.5</td><td>0.0</td><td>2.1</td><td>14.6</td>
            </tr>

            <tr><td colspan="12" class="model-category">Open-Source MLLMs</td></tr>
            <tr>
              <td>Llama-3.2-90B-Vision-Ins</td>
              <td>30.0</td><td>48.3</td><td>23.3</td><td>38.3</td><td>33.3</td>
              <td>6.7</td><td>14.9</td><td>10.4</td><td>12.5</td><td>16.7</td><td>10.4</td>
            </tr>
            <tr>
              <td>Llama-3.2-11B-Vision-Ins</td>
              <td>21.4</td><td>23.3</td><td>21.7</td><td>26.7</td><td>18.3</td>
              <td>17.0</td><td>0.9</td><td>0.0</td><td>2.1</td><td>0.0</td><td>2.1</td>
            </tr>
            <tr>
              <td>InternVL2 5-78B</td>
              <td>30.7</td><td>36.7</td><td>38.3</td><td>33.3</td><td>21.7</td>
              <td>23.3</td><td>18.0</td><td>16.7</td><td>14.6</td><td>22.2</td><td>20.8</td>
            </tr>
            <tr>
              <td>InternVL2 5-38B</td>
              <td>30.3</td><td>35.0</td><td>28.3</td><td>38.3</td><td>26.7</td>
              <td>23.3</td><td>15.8</td><td>22.9</td><td>8.3</td><td>13.9</td><td>16.7</td>
            </tr>
            <tr>
              <td>InternVL2 5-8B</td>
              <td>21.3</td><td>35.0</td><td>23.3</td><td>21.7</td><td>26.7</td>
              <td>0.0</td><td>7.0</td><td>8.3</td><td>2.1</td><td>6.3</td><td>10.4</td>
            </tr>
            <tr>
              <td>Qwen2-VL-72B-Ins</td>
              <td>21.2</td><td>26.7</td><td>30.0</td><td>28.3</td><td>16.0</td>
              <td>5.0</td><td>13.6</td><td>18.8</td><td>20.8</td><td>4.2</td><td>14.6</td>
            </tr>
            <tr>
              <td>Qwen2-VL-7B-Ins</td>
              <td>14.0</td><td>26.7</td><td>10.0</td><td>15.0</td><td>15.0</td>
              <td>3.3</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td>
            </tr>
          </tbody>
        </table>
        <figcaption class="figure-caption">
          <b>Table 3.</b> Main results of EB-Navigation and EB-Manipulation on 5 subsets with Proprietary Models and Open-Source Models.
        </figcaption>
      </figure>

      <!-- Paragraph after tables -->
      <p style="text-align:justify; margin-top:1em; color:black;">
        Claude-3.5-Sonnet excels in <b>high-level</b> tasks (EB-ALFRED, EB-Habitat) at 64–68%, while GPT-4o leads
        <b>low-level</b> tasks (EB-Navigation at 57.7%, EB-Manipulation at 28.9%). Among open-source models,
        InternVL2.5 (78B) outperforms smaller Llama-3.2 Vision or Qwen2-VL but still trails top proprietary solutions.
        Removing visual input drastically reduces success in <b>low-level</b> tasks (e.g., GPT-4o’s navigation:
        57.7% → 17.4%), whereas high-level tasks often remain text-driven. Long-horizon planning (15+ steps) cuts
        performance by 20–30%, underscoring the need for robust <b>visual</b> <b>grounding</b> and action-level
        <b>awareness</b>. Advanced <b>visual</b> <b>cues</b>, including bounding boxes and <b>visual</b>
        <b>in-context</b> <b>learning</b>, significantly bolster object localization and action grounding,
        whereas <b>multi-step</b> or <b>multi-view</b> image inputs frequently <b>confuse</b> current MLLMs—
        underscoring the need for more sophisticated integration strategies when expanding the agent’s visual context.
      </p>

      <!-- Figure 6. Error Analysis -->
      <figure class="table-figure" style="margin-top:1em;">
        <img class="error-analysis-img" src="website/img/error_analysis.png" alt="Figure 6. Error Analysis." />
        <figcaption class="figure-caption">
          <b>Figure 6.</b> Error Analysis.
        </figcaption>
      </figure>

      <p style="text-align:justify; margin-top:1em; color:black;">
        In <b>EB-ALFRED</b>, <b>planning</b> (55.3%) and <b>reasoning</b> (41.3%) errors dominate, revealing the model’s
        tendency to miss steps, issue invalid actions, or end tasks prematurely. By contrast, <b>EB-Manipulation</b>
        shows a <b>33.3%</b> <b>perception</b> error rate—often due to wrong recognition—indicating that GPT-4o struggles
        to robustly identify objects and precisely estimate gripper poses, even with bounding-box aids.
      </p>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="BibTeX" style="padding-top:1.5rem;">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
@inproceedings{Yang2025embodied,
  title     = {EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents},
  author    = {Yang, Rui and Chen, Hanyang and Zhang, Junyu and Zhao, Mark and Qian, Cheng and Wang, Kangrui and Wang, Qineng and Koripella, Teja Venkat and Movahedi, Marziyeh and Li, Manling and Ji, Heng and Zhang, Huan and Zhang, Tong},
  booktitle = {ICML 2025},
  year      = {2025}
}
      </pre>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer" style="padding:1rem 1.5rem;">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content is-small">
          This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>
</html>

