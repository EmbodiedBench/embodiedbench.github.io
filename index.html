<!DOCTYPE html>
<html>
<head>
  <title>EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</title>

  <!-- Favicon -->
  <link rel="icon" href="website/img/embodied-logo.png" type="image/png">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Chart.js and plugins -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

  <!-- Bulma & Additional CSS (Google Sans, Noto Sans) -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="website/css/bulma.min.css">
  <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="website/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="website/css/fontawesome.all.min.css">

  <!-- jQuery & Bulma scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="website/javascript/bulma-slider.min.js"></script>
  <script src="website/javascript/bulma-carousel.min.js"></script>

  <!-- Bootstrap 5 -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
        crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
          crossorigin="anonymous"></script>

  <!-- Tabulator -->
  <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
  <script defer src="website/javascript/fontawesome.all.min.js"></script>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="website/css/index.css">

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-C7GJ4FYMY9');
  </script>

  <!-- MathJax -->
  <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/javascript">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <noscript>
    <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
  </noscript>

  <!-- Toggling + Carousel Setup + Table Loading -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      //--- Toggle sections
      var toggles = document.querySelectorAll('.toggle-section');
      toggles.forEach(function(toggle){
        toggle.addEventListener('click', function(){
          var content = document.getElementById(toggle.getAttribute('aria-controls'));
          var toggleIcon = toggle.children[1].children[0];
          content.classList.toggle('is-active');
          if(content.classList.contains('is-active')) {
            toggleIcon.style.transition = 'transform 0.3s ease';
            toggleIcon.style.transform = 'rotate(180deg)';
          } else {
            toggleIcon.style.transition = 'transform 0.3s ease';
            toggleIcon.style.transform = 'rotate(0deg)';
          }
        });
      });

      //--- Carousel config: no-lag, no-wrap
      function configureCarousel(carouselId) {
        let carouselElem = document.getElementById(carouselId);
        if(!carouselElem) return;
        let carousel = new bootstrap.Carousel(carouselElem, {
          interval: false,
          wrap: false
        });
        let items = carouselElem.querySelectorAll('.carousel-item');
        let total = items.length;

        function updateArrows(index) {
          let prevBtn = carouselElem.querySelector('.carousel-control-prev');
          let nextBtn = carouselElem.querySelector('.carousel-control-next');
          // Hide prev if first
          if(index === 0) prevBtn.style.display = 'none';
          else prevBtn.style.display = 'block';
          // Hide next if last
          if(index === total - 1) nextBtn.style.display = 'none';
          else nextBtn.style.display = 'block';
        }
        carouselElem.addEventListener('slid.bs.carousel', e => updateArrows(e.to));
        updateArrows(0);
      }
      configureCarousel('successCarousel');
      configureCarousel('failureCarousel');

      //--- Load Table 2 from JSON
      fetch("website/data/table2_alfred_habitat.json")
        .then(response => response.json())
        .then(data => {
          new Tabulator("#table2-alfred-habitat", {
            data: data,
            layout: "fitData",
            columns: [
              {title: "Category", field: "category", widthGrow:1},
              {title: "Model", field: "model", widthGrow:2},
              {title: "AL_Avg", field: "AL_Avg"},
              {title: "AL_Base", field: "AL_Base"},
              {title: "AL_Common", field: "AL_Common"},
              {title: "AL_Complex", field: "AL_Complex"},
              {title: "AL_Visual", field: "AL_Visual"},
              {title: "AL_Spatial", field: "AL_Spatial"},
              {title: "AL_Long", field: "AL_Long"},
              {title: "HB_Avg", field: "HB_Avg"},
              {title: "HB_Base", field: "HB_Base"},
              {title: "HB_Common", field: "HB_Common"},
              {title: "HB_Complex", field: "HB_Complex"},
              {title: "HB_Visual", field: "HB_Visual"},
              {title: "HB_Spatial", field: "HB_Spatial"},
              {title: "HB_Long", field: "HB_Long"},
            ],
          });
        });

      //--- Load Table 3 from JSON
      fetch("website/data/table3_nav_mani.json")
        .then(response => response.json())
        .then(data => {
          new Tabulator("#table3-nav-mani", {
            data: data,
            layout: "fitData",
            columns: [
              {title: "Category", field: "category", widthGrow:1},
              {title: "Model", field: "model", widthGrow:2},
              {title: "Nav_Avg", field: "Nav_Avg"},
              {title: "Nav_Base", field: "Nav_Base"},
              {title: "Nav_Common", field: "Nav_Common"},
              {title: "Nav_Complex", field: "Nav_Complex"},
              {title: "Nav_Visual", field: "Nav_Visual"},
              {title: "Mani_Avg", field: "Mani_Avg"},
              {title: "Mani_Base", field: "Mani_Base"},
              {title: "Mani_Common", field: "Mani_Common"},
              {title: "Mani_Complex", field: "Mani_Complex"},
              {title: "Mani_Visual", field: "Mani_Visual"},
              {title: "Mani_Spatial", field: "Mani_Spatial"},
            ],
          });
        });

    });
  </script>

  <style>
    /* Font families & base size from sample code */
    html, body {
      font-family: 'Noto Sans', sans-serif;
      font-size: 1.1rem;
    }
    h1, h2, h3, h4, .title, .subtitle {
      font-family: 'Google Sans', sans-serif;
    }

    /* No-lag transitions in carousel */
    .carousel-item {
      transition: none !important;
    }
    .carousel-inner {
      transition: none !important;
    }

    /* Fix arrow positions */
    .carousel-control-prev,
    .carousel-control-next {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      width: 5%;
    }
    .carousel-control-prev { left: 0.5rem; }
    .carousel-control-next { right: 0.5rem; }

    /* Use black triangle arrow icons */
    .carousel-control-prev-icon,
    .carousel-control-next-icon {
      background: none;
      width: 40px;
      height: 40px;
      border-radius: 0;
    }
    .carousel-control-prev-icon {
      background-image: url("data:image/svg+xml;utf8,%3Csvg fill='black' height='40' width='40' viewBox='0 0 8 8' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M 5.5 0 L 0 4 L 5.5 8 z' /%3E%3C/svg%3E");
      background-size: contain;
      background-repeat: no-repeat;
    }
    .carousel-control-next-icon {
      background-image: url("data:image/svg+xml;utf8,%3Csvg fill='black' height='40' width='40' viewBox='0 0 8 8' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M 2.5 0 L 8 4 L 2.5 8 z' /%3E%3C/svg%3E");
      background-size: contain;
      background-repeat: no-repeat;
    }

    /* Restrict carousel images to 600px => bigger than before */
    .carousel-item img {
      max-width: 600px;
      margin: 0 auto;
      display: block;
    }

    /* Collapsible content from sample */
    .collapse-content {
      display: none;
      margin-top: 10px;
    }
    .collapse-content.is-active {
      display: block;
    }

    /* Center the image subtitles & table captions */
    .image-subtitle {
      text-align: center;
      font-style: italic;
      margin-top: 0.3rem;
      margin-bottom: 1rem;
    }
    .figure-caption {
      text-align: center;
      font-style: italic;
      margin-top: 0.3rem;
      margin-bottom: 1rem;
      color: #333;
    }

    /* Style placeholders for Tabulator tables if needed */
    #table2-alfred-habitat, #table3-nav-mani {
      margin-top: 1rem;
    }
  </style>
</head>

<body>

  <!-- MAIN TITLE / HERO -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 is-bold" style="text-align:center;">
              <img src="website/img/embodied-logo.png"
                   alt="logo"
                   style="vertical-align: middle; margin-right: 10px;"
                   width="90" height="90" />
              EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents
            </h1>
            <!-- Authors & Affiliations -->
            <div class="is-size-5 publication-authors" style="margin-top:1.5rem;">
              <span class="author-block">
                <a href="https://yangrui2015.github.io/">Rui Yang</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Hanyang_Chen3">Hanyang Chen</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jyzhang1208.github.io/">Junyu Zhang</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/markzihaozhao/">Mark Zhao</a><sup>*3</sup>,
              </span>
              <span class="author-block">
                <a href="https://qiancheng0.github.io/">Cheng Qian</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jameskrw.github.io/">Kangrui Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://qinengwang-aiden.github.io/">Qineng Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/teja-koripella/">Teja Venkat Koripella</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/marziyeh-movahedi-439b7284/">Marziyeh Movahedi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://limanling.github.io/">Manling Li</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://blender.cs.illinois.edu/hengji.html">Heng Ji</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.huan-zhang.com/">Huan Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://tongzhang-ml.org/">Tong Zhang</a><sup>1</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors" style="margin-top:1em;">
              <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign (UIUC),</span>
              <span class="author-block"><sup>2</sup>Northwestern University,</span>
              <span class="author-block"><sup>3</sup>University of Toronto</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution</span>
            </div>

            <!-- Arxiv & Code Links -->
            <div class="column has-text-centered" style="margin-top:1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.07166" class="btn btn-outline-dark" role="button" target="_blank">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>&nbsp;&nbsp;
              </span>
              <span class="link-block">
                <a href="https://github.com/embodied-agent-eval/embodied-agent-eval" class="btn btn-outline-dark" role="button" target="_blank">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>&nbsp;&nbsp;
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser Video Section -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body" style="padding-top:1rem;">
        <video id="teaser" autoplay muted loop playsinline controls width="100%">
          <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-0-overview.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section>

  <!-- 4 short videos section -->
  <section class="section" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h3 class="title is-5">Goal Interpretation</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-1-goal.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Subgoal Decomposition</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-2-subgoal.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Action Sequencing</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-3-action.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <h3 class="title is-5">Transition Modeling</h3>
          <video autoplay muted loop playsinline controls width="100%">
            <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/website/videos/eai-4-transition.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered has-text-centered" style="margin-top:1rem;">
        <div class="column">
          <button class="button is-light toggle-section" aria-controls="full_demo_video">
            <span class="icon"><i class="fas fa-play"></i></span>
            <span>Watch Full Demo</span>
            <span class="icon"><i class="fas fa-angle-down" style="margin-left:5px;"></i></span>
          </button>
          <div id="full_demo_video" class="collapse-content" style="margin-top:1rem;">
            <video id="demo" controls width="100%">
              <source src="https://github.com/embodied-agent-eval/embodied-agent-eval.github.io/raw/main/eai.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract Section -->
  <section class="section" id="abstract" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align:center;">Abstract</h2>
      <div class="columns is-centered">
        <div class="column is-eight-fifths">
          <div class="content has-text-justified">
            <p>
              Leveraging <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(MLLMs)</b> to create embodied agents
              offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have
              garnered substantial attention, <b>MLLM-based embodied agents remain underexplored</b> due to the lack of
              comprehensive evaluation frameworks. To bridge this gap, we introduce <b>EmbodiedBench</b>, an extensive
              benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of
              <b>1,128 testing tasks</b> across four environments, ranging from high-level semantic tasks (e.g., household)
              to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2)
              <b>six meticulously curated subsets</b> evaluating essential agent capabilities like common sense reasoning,
              complex instruction following, spatial awareness, visual perception, and long-term planning. Through extensive
              experiments, we evaluate <b>13 leading proprietary and open-source MLLMs within EmbodiedBench</b>. Our findings
              reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model,
              GPT-4o, scoring only on average. <b>EmbodiedBench provides a standardized, multifaceted evaluation platform</b>
              that not only highlights existing challenges but also offers valuable insights for advancing MLLM-based
              embodied agents.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Overview Section -->
  <section class="section" id="overview" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align:center; margin-bottom:1rem;">Overview</h2>

      <div class="columns is-centered has-text-centered">
        <div class="column is-eight-fifths">
          <figure class="table-figure">
            <img class="figure-image" src="website/img/framework_overview.png" alt="Figure 1. Overview of EMBODIEDBENCH." />
            <figcaption class="figure-caption">
              <b>Figure 1.</b> Overview of EMBODIEDBENCH. Two key features of our benchmark: various action levels and capability-oriented evaluation.
            </figcaption>
          </figure>

          <p style="text-align:justify;">
            We propose <b>EmbodiedBench</b> to bridge the gap between text-centric benchmarks and <b>real-world</b>,
            <b>vision-driven</b> tasks. Our benchmark evaluates <b>Multi-modal</b> <b>Large</b> <b>Language</b>
            <b>Models</b> <b>(MLLMs)</b> on both <b>high-level</b> semantic planning (e.g., “find a tomato and place it in the sink”)
            and <b>low-level</b> atomic actions (e.g., discretized robot arm actions or atomic movement commands). By unifying
            <b>multiple</b> <b>action</b> <b>levels</b> and <b>capability-based</b> <b>subsets</b> in a single platform,
            EmbodiedBench offers a <b>direct comparison</b> of how effectively MLLMs tackle real-world complexity,
            ranging from everyday semantic tasks to <b>fine-grained</b> atomic commands, all while leveraging
            <b>vision</b> as a key input. Our extensive experiments with 13 state-of-the-art models reveal specific strengths
            (e.g., high-level planning) and consistent weaknesses (especially in low-level manipulation and long-horizon tasks).
          </p>

          <p style="text-align:justify; margin-top:1em;">
            <b>EmbodiedBench</b> is a <b>vision-driven</b> benchmark that enforces <b>hierarchical</b> <b>action</b>
            <b>representation</b> in MLLMs, from <b>high-level</b> commands (e.g.,
            <span style="color:green; font-style: italic;">"find a HandTowel"</span>)
            to <b>low-level</b> 7D vectors (<span style="color:green; font-style: italic;">[X, Y, Z, Roll, Pitch, Yaw, Gripper]</span>).
            Its four environments, <b>EB-ALFRED</b>, <b>EB-Habitat</b>, <b>EB-Navigation</b>, and <b>EB-Manipulation</b>,
            span tasks ranging from abstract household directives to precise robotic control. Each environment’s tasks
            are organized into six <b>capability-oriented</b> subsets (Base, Common Sense, Complex Instruction, Spatial Awareness,
            Visual Appearance, Long Horizon). Altogether, 13<b> leading</b> <b>MLLMs</b>, including <b>GPT-4o</b>,
            <b>Claude-3.5-Sonnet</b>, <b>Gemini</b>, <b>Llama-3.2</b> <b>Vision</b>, <b>InternVL2.5</b>, and <b>Qwen2-VL</b>,
            are evaluated, providing a comprehensive look at how well these models handle <b>semantic</b> <b>planning</b>
            and <b>fine-grained</b> manipulation in real-world-inspired scenarios.
          </p>
        </div>
      </div>

      <div class="columns is-centered has-text-centered" style="margin-top:2em;">
        <div class="column is-eight-fifths">
          <figure class="table-figure">
            <img class="figure-image" src="website/img/planner_pipeline.png" alt="Figure 2. Vision-driven agent pipeline used in EMBODIEDBENCH." />
            <figcaption class="figure-caption">
              <b>Figure 2.</b> Vision-driven agent pipeline used in EMBODIEDBENCH.
            </figcaption>
          </figure>

          <p style="text-align:justify; margin-top:1em;">
            Our <b>vision-driven</b> pipeline fuses visual perception, language instructions, and in-context examples
            to guide <b>MLLM-based</b> agents in both <b>high-level</b> and <b>low-level</b> tasks. The decision module
            sequentially processes (1) the current egocentric image, (2) the user goal, (3) a valid skill or action format,
            and (4) in-context demonstration, yielding a <b>Language Plan</b> that’s converted into an <b>Executable</b>
            <b>Plan</b>. After each execution, the environment’s feedback (collisions, success signals) triggers reflection
            and potential replanning, enabling a single model to handle everything from <b>semantic</b> tasks to
            <b>fine-grained</b> robotic controls. In <b>Figure</b> <b>3</b>, you can see two examples of our planner.
          </p>
        </div>
      </div>

      <div class="columns is-centered has-text-centered" style="margin-top:2em;">
        <div class="column is-eight-fifths">
          <figure class="table-figure">
            <img class="figure-image" src="website/img/planning_example.png" alt="Figure 3. Planner Examples." />
            <figcaption class="figure-caption">
              <b>Figure 3.</b> Planning examples in EB-ALFRED and EB-Manipulation based on GPT-4o.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

  <!-- Leaderboard and Findings -->
  <section class="section" id="leaderboard-findings" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align:center; margin-bottom:1rem;">Leaderboard and Findings</h2>
      <div style="height:1rem;"></div>

      <!-- Table 2 placeholder (Tabulator loads from JSON) -->
      <figure class="table-figure">
        <div id="table2-alfred-habitat"></div>
        <figcaption class="figure-caption">
          <b>Table 2.</b> Main results of EB-ALFRED and EB-HABITAT on 6 subsets with proprietary models and open-source models.<br>
          The best proprietary model is in <b>bold</b>, and the best open-source model is in <b><u>bold underline</u></b>.
        </figcaption>
      </figure>

      <br><!-- Blank line between Table 2 and Table 3 -->

      <!-- Table 3 placeholder (Tabulator loads from JSON) -->
      <figure class="table-figure">
        <div id="table3-nav-mani"></div>
        <figcaption class="figure-caption">
          <b>Table 3.</b> Main results of EB-Navigation and EB-Manipulation on 5 subsets with Proprietary Models and Open-Source Models.
        </figcaption>
      </figure>

      <p style="text-align:justify; margin-top:1em;">
        <b>Claude-3.5-Sonnet</b> excels in <b>high-level</b> tasks (EB-ALFRED, EB-Habitat) at 64–68%, while <b>GPT-4o</b> leads
        <b>low-level</b> tasks (EB-Navigation at 57.7%, EB-Manipulation at 28.9%). Among open-source models, InternVL2.5 (78B)
        outperforms smaller Llama-3.2 Vision or Qwen2-VL but still trails top proprietary solutions. Removing visual input
        drastically reduces success in <b>low-level</b> tasks (e.g., GPT-4o’s navigation: 57.7% → 17.4%), whereas high-level
        tasks often remain text-driven. Long-horizon planning (15+ steps) cuts performance by 20–30%, underscoring the need
        for robust <b>visual</b> <b>grounding</b> and action-level <b>awareness</b>. Advanced <b>visual</b> <b>cues</b>,
        including bounding boxes and <b>visual</b> <b>in-context</b> <b>learning</b>, significantly bolster object
        localization and action grounding, whereas <b>multi-step</b> or <b>multi-view</b> image inputs frequently
        <b>confuse</b> current MLLMs—underscoring the need for more sophisticated integration strategies when expanding
        the agent’s visual context.
      </p>

      <figure class="table-figure" style="margin-top:1em;">
        <img class="figure-image" src="website/img/error_analysis.png" alt="Figure 6. Error Analysis." />
        <figcaption class="figure-caption">
          <b>Figure 6.</b> Error Analysis.
        </figcaption>
      </figure>

      <p style="text-align:justify; margin-top:1em;">
        In <b>EB-ALFRED</b>, <b>planning</b> (55.3%) and <b>reasoning</b> (41.3%) errors dominate, revealing the model’s
        tendency to miss steps, issue invalid actions, or end tasks prematurely. By contrast, <b>EB-Manipulation</b>
        shows a <b>33.3%</b> <b>perception</b> error rate—often due to wrong recognition—indicating that GPT-4o struggles
        to robustly identify objects and precisely estimate gripper poses, even with bounding-box aids.
      </p>
    </div>
  </section>

  <!-- Case Study -->
  <section class="section" id="case-study" style="padding-top:1.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align:center; margin-bottom:1rem;">Case Study</h2>
      <br>
      <!-- Success Carousel -->
      <h3 class="title is-4" style="text-align:center;">Success Examples</h3>
      <div id="successCarousel" class="carousel" data-bs-interval="false" data-bs-wrap="false" style="margin-bottom:2rem;">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <img src="website/img/success_alfred_claude.png" alt="success_alfred_claude" />
            <p class="image-subtitle">
              <b>Figure 13.</b> Planning example in EB-AFRED for Claude-3.5-Sonnet.
            </p>
          </div>
          <div class="carousel-item">
            <img src="website/img/success_habitat_internvl.png" alt="success_habitat_internvl" />
            <p class="image-subtitle">
              <b>Figure 14.</b> Planning example in EB-Habitat for InternVL2.5-78B.
            </p>
          </div>
          <div class="carousel-item">
            <img src="website/img/success_mani_gemini.png" alt="success_mani_gemini" />
            <p class="image-subtitle">
              <b>Figure 16.</b> Planning example in EB-Manipulation for Gemini-1.5-Pro.
            </p>
          </div>
          <div class="carousel-item">
            <img src="website/img/success_nav_gpt4o.png" alt="success_nav_gpt4o" />
            <p class="image-subtitle">
              <b>Figure 15.</b> Planning example in EB-Navigation for GPT-4o.
            </p>
          </div>
        </div>
        <button class="carousel-control-prev" type="button" data-bs-target="#successCarousel" data-bs-slide="prev">
          <span class="carousel-control-prev-icon" aria-hidden="true"></span>
          <span class="visually-hidden">Previous</span>
        </button>
        <button class="carousel-control-next" type="button" data-bs-target="#successCarousel" data-bs-slide="next">
          <span class="carousel-control-next-icon" aria-hidden="true"></span>
          <span class="visually-hidden">Next</span>
        </button>
      </div>

      <!-- Failure Carousel -->
      <h3 class="title is-4" style="text-align:center;">Failure Examples</h3>
      <div id="failureCarousel" class="carousel" data-bs-interval="false" data-bs-wrap="false">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <img src="website/img/perception_error_mani.png" alt="perception_error_mani" />
            <p class="image-subtitle">
              <b>Figure 18.</b> Perception Error Example in EB-Manipulation.
            </p>
          </div>
          <div class="carousel-item">
            <img src="website/img/planning_error_alfred.png" alt="planning_error_alfred" />
            <p class="image-subtitle">
              <b>Figure 17.</b> Planning Error Example in EB-ALFRED.
            </p>
          </div>
          <div class="carousel-item">
            <img src="website/img/reasoning_error_nav.png" alt="reasoning_error_nav" />
            <p class="image-subtitle">
              <b>Figure 19.</b> Reasoning Error Example in EB-Navigation.
            </p>
          </div>
        </div>
        <button class="carousel-control-prev" type="button" data-bs-target="#failureCarousel" data-bs-slide="prev">
          <span class="carousel-control-prev-icon" aria-hidden="true"></span>
          <span class="visually-hidden">Previous</span>
        </button>
        <button class="carousel-control-next" type="button" data-bs-target="#failureCarousel" data-bs-slide="next">
          <span class="carousel-control-next-icon" aria-hidden="true"></span>
          <span class="visually-hidden">Next</span>
        </button>
      </div>
    </div>
  </section>

  <!-- BibTeX Section -->
  <section class="section" id="BibTeX" style="padding-top:1.5rem;">
    <div class="container is-max-desktop content">
      <h2 class="title is-2" style="text-align:center;">BibTeX</h2>
      <pre>
@inproceedings{Yang2025embodied,
  title     = {EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents},
  author    = {Yang, Rui and Chen, Hanyang and Zhang, Junyu and Zhao, Mark and Qian, Cheng and Wang, Kangrui and Wang, Qineng and Koripella, Teja Venkat and Movahedi, Marziyeh and Li, Manling and Ji, Heng and Zhang, Huan and Zhang, Tong},
  booktitle = {ICML 2025},
  year      = {2025}
}
      </pre>
    </div>
  </section>

  <footer class="footer" style="padding:1rem 1.5rem;">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content is-small">
          This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
